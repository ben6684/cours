\documentclass[11pt,twoside,a4paper]{report}
%=========================== En-Tete =================================
%--- Insertion de paquetages (optionnel) ---
\usepackage[french]{babel}   % pour dire que le texte est en franÃ§ais
\usepackage{a4}              % pour la taille   
\usepackage[latin1]{inputenc}     % pour les font postscript
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{amsmath} 
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}

\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{myred}{rgb}{0.6,0,0}
\definecolor{myblue}{rgb}{0,0,0.6}

\begin{document}
\lstset{%
backgroundcolor=\color{white}, 
basicstyle=\footnotesize,
commentstyle=\color{mygreen},
stringstyle=\color{myred},
breaklines=true,
frame=shadowbox,
keepspaces=true,
columns    = flexible,%
keywordstyle=\color{myblue},
language=Python,
numbers=left,
numbersep=5pt,
numberstyle=\tiny\color{black},
stringstyle=\color{mymauve}, 
xleftmargin=\parindent,
tabsize=3,
}%

\chapter{TP n°2}

\section{Problématique}


La manière de faire des requêtes de type SQL sur des ensembles de données non-structuré a été abordé dans le chapitre précédent et a posé les bases de l'écriture d'algorithme MapReduce.\\

\section{Bases de statistique}

\subsection{Exercice 1}

Calcul de le prix de la vente moyenne.\\

En SQL :\\
 
\begin{lstlisting}
SELECT AVG(prix) FROM client;
\end{lstlisting}
 
En Math :\\
\begin{align}
\overline{X} = \frac{1}{n} \sum_{i=1}^{n} x_i
\end{align}

Indication : Vous devez connaître la somme des prix ET le nombre de vente

\subsubsection{Corrigé}

Plusieurs solutions sont possibles : \\

\begin{itemize}
\item la première solution est une solution qui n'est pas MapReduce. Pour cela il suffit d'envoyer en sortie du mapper la clef nulle, pour récupérer sur un seul reduceur la liste des valeurs. Ainsi il suffit de sommer les valeurs et de la diviser par la taille de la liste.\\

\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			prix = l.split(',')[2]
			yield None,float(prix)

	def reducer(self, _, v):
		l = list(v)
		print sum(l)/len(l)
\end{lstlisting}

une autre possibilité pour le reducer: \\

\begin{lstlisting}
	def reducer(self, _, v):
        count = somme = 0
        for i in l:
            count =+ 1
            somme += i[1]
		print somme/count
\end{lstlisting}

Cette solution n'est pas MapReduce malgré toutes les apparences. Elle ne peut donc pas être considérée comme valide.\\

\item La deuxième solution est d'utiliser un combiner qui est une étape supplémentaire dans un processus MapReduce qui se place entre le mapper et le reducer et qui a pour but de simplifier la tache du reducer. En effet, il est d'usage de mettre moins de reducer que de mapper et donc d'avoir plus de machines dédiées à l'étape du mapper. Mais comme une grande partie des fonction d'aggrégations se font coté reducer, il devient difficile de calculer des fonctions en temps raisonnable. Ainsi pour simplifier la tache on utilise un combiner, qui prend en entrée une clef et la liste des valeurs du mapper (et non de tous les mapper). Ce combiner va donc commencer la tache du reducer et envoyer moins de valeurs. \\

Pa rexemple sur l'exemple du wordcount : 

\begin{lstlisting}
	def mapper(self, _, line):
		words = line.split(',')
		for w in words:
			yield(w,1)

    #ici pour chaque mapper on commence à sommer les mots
    def combiner(self,k,v):
        yield k, sum(v)

	def reducer(self, k, v):
		yield k, sum(v)
\end{lstlisting}

chaque mapper ayant une partie des données, au lieu d'envoyer <mot, (1,1,1,1,1,1,1,1,1,1,1,1,1)> le combiner enverra <mot, 13> et ainsi en entrée de chaque reducer il n'y aura pas une suite longue de 1 qu'il faudra sommer mais une courte liste de valeur.\\

Ainsi pour l'exemple de la moyenne :\\

\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			yield None, (1,float(l.split(',')[2]))
			
	def combiner(self,_,v):
		c,s = zip(*v)
		yield None, (sum(c),sum(s))

	def reducer(self, _, v):
		c,s = zip(*v)
		print sum(s)/sum(c)
\end{lstlisting}

La fonction zip permet d'extraire les colonnes d'une liste de liste.\\

\item La troisième est d'enchainer deux processus map-reduce

\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			yield "count",1
			yield "somme",float(l.split(',')[2])
			
	def combiner(self,k,v):
		yield k, sum(v)
		
	def reducer(self, k, v):
		yield None, (k,sum(v))

	def reducer_2(self,_,v):
		l = list(v)
		print l[1][1]/l[0][1]
\end{lstlisting}

cette solution permet d'utiliser deux réducers pour sommer les 1 et les valeurs qui sont ensuite envoyer à un deuxième reducer. La somme est sut comme deuxième élément car SORT\_VALUES = True se qui implique que les valeurs sont triées elles aussi et que ce trie s'effectue sur la première colonne. \\

\end{itemize}

\subsection{Exercice 2}

Calculer l'écart type des achats totaux par client.

Pour rappelle l'écart type d'une variable aléatoire discrète X se calcul :

\begin{align}
  \sigma(X) = \sqrt{\frac{1}{n} \sum_{i=1}^{n}(x_i - \overline{X})^2} 
\end{align}


Les informations nécessaires sont : 
\begin{itemize}
\item la somme des achats par client (qui devient notre variable aléatoire $X$)
\item la moyenne
\item le nombre de client
\end{itemize}

\subsubsection{Corrigé}

\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			yield l.split(',')[0], float(l.split(',')[2])
			
	def combiner(self,k,v):
		yield k,sum(v)

	def reducer(self, k, v):
		yield None, sum(v)
		
	def reducer_2(self, _, v):
		X = list(v)
		m = sum(X)/len(X)
		print sqrt(sum([(x-m)*(x-m) for x in X])/len(X))
\end{lstlisting}

Remarque, ici les sommes sont effectué dans un deuxième reducer. Cela avait été interdit pour la moyenne, car le parcours devait se faire sur une très grande données (nombre de vente), alors qu'ici il s'agit d'une sous-liste ne contenant que le nombre de client, ce qui est beaucoup moins. \\

Remarque sqrt est disponible dans le package math de python mais vous pourriez aussi d'utiliser numpy. 

\subsection{Multiplication de matrices}

Soit deux Matrices de grandes tailles qui doivent êtres multipliés entre elles. Ecrire l'algorithme MapReduce qui permet de multiplier deux matrices. Pour cela, deux fichiers seront utiliser ayant pour structure $v_{0,0}$,$v_{0,1}$,...,$v_{0,m}\backslash$n ... jusqu'à la dernière ligne.\\

Indication : On veut en sorti un fichier qui ai la même structuration que les fichiers d'entrées.\\

Pour charger deux fichiers dans deux structures différentes, il faut pouvoir spécifier des options dans notre ligne de commande. Pour cela il suffit de surcharger la méthode "configure_options":

\begin{lstlisting}
import pandas as pd

    def configure_options(self):
        super(MRMultMat, self).configure_options()
        self.add_passthrough_option('--m1', help="Filename of matrix 1")
        self.add_passthrough_option('--m2', help="Filename of matrix 2")

	def mapper(self, _, lines):
		m1 = pd_read_csv(self.options.m1)
		m2 = pd_read_csv(self.options.m2)
\end{lstlisting}

\subsubsection{Corrigé}

\begin{lstlisting}
	def mapper(self, _, lines):
		
		m1 = pd.read_csv(self.options.m1,names=range(100),header=0)
		m2 = pd.read_csv(self.options.m2,names=range(100),header=0)
		
		for i in range(100):
			l = m1.ix[i].tolist()
			for j in range(100):
				c = m2[j].tolist()
				yield (i,j, sum([ii*jj for ii,jj in zip(l,c)])), None
				
	def reducer(self,k,_):
		yield None, k
		
	def reducer_2(self, _, v):
		for vv in v:
			i, j, vvv = vv
			print i,j, vvv
\end{lstlisting}

Attention pour lancer la commande, MRJob pour se conformer à Haddop oblige que les valeurs soient envoyées via STDIN, voilà pourquoi un fichier toto.txt a été créer n'ayant pour contenu que 0. \\

La ligne de commande devient alors : \\

\begin{lstlisting}
python MRMultMat.py -r local --m1 ~/data/downloads/MRjob/mat1.csv --m2 ~/data/downloads/MRjob/mat2.csv < toto.txt
\end{lstlisting}


\subsection{k-means}

Comment partitionner en trois ensembles de données, une liste de points en deux dimensions via un algorithme de k-means.

\subsubsection{algorithme}

Pour faire simple voici ce qu'en dit Wikipedia :\\

\begin{quote}
"L'algorithme des k-moyennes (ou K-means en anglais) est un algorithme de partitionnement de données relevant des statistiques et de l'apprentissage automatique (plus précisément de l'apprentissage non supervisé). C'est une méthode dont le but est de diviser des observations en K partitions (clusters) dans lesquelles chaque observation appartient à la partition avec la moyenne la plus proche."\\
\end{quote}

Soit un ensemble de point $X=(x_1,x_2, ...,x_n)$ où les $x_i$ représente les vecteurs d'observation de dimension $d$. L'algorithme cherche à minimiser la distance entre chaque point de chaque $k$ ensemble d'observations $S$.\\

\begin{align}
\underset{\mathbf{S}}{\operatorname{arg\,min}} \sum_{i=1}^{k} \sum_{\mathbf x_j \in S_i} \left\| \mathbf x_j - \boldsymbol\mu_i \right\|^2 
\end{align}

Où $\boldsymbol\mu_i$ est la moyenne des points dans $S_i$\\

\subsubsection{Indications}

Pour résoudre un algorithme k-means dans un cadre non BigData, il suffit d'une phase d'initialisation, permettant de choisir $k$-points aléatoirement qui seront nos centroïdes $\boldsymbol\mu_i$. Ensuite on applique une suite d'instructions tant qu'il n'y a pas convergence. Ces étapes sont, ccalculer pour chaque cluster les nouveaux centroïdes puis de vérifier pour chaque point à quelle cluster ils appartiennent.\\

Cette algorithme ne nécessite que deux étapes :
\begin{itemize}
\item la labélisation de chaque point (à quelle cluste il appartient)
\item le calcul de la moyenne des points.
\end{itemize}

Un autre point très important est la possibilité d'utiliser dans un programme Python la librairie numpy, extrement pratique, notemment pour calculer la distance entre vecteur.\\

Maintenant en utilisant un langage plus MapReduce que nous donne ces deux étapes : 
\begin{itemize}
\item Pour chaque point : je calcul une distance avec tous les clusters dont je tirerais le minimum.
\item Pour chaque point d'un cluster, je fait la moyenne.
\end{itemize}

L'utilisation des mapper et reducer et leurs entrées-sorties doivent vous guider vers une première étape (moyenne faite dans un seul réducer par exemple).\\

De plus il faut apprendre à sauvegarder des valeurs (un compteur, une liste) entre les différentes étapes de votre class MRJob, pour cela il suffit de surcharger la méthod init de votre class pour pouvoir y rajouter des variables de classes:

\begin{lstlisting}
class MRKMeans(MRJob):

	SORT_VALUES = True

	def __init__(self, *args, **kwargs):
		super(MRKMeans, self).__init__(*args, **kwargs)
		old_centroids = []
		new_centroid=[]
\end{lstlisting}





\subsubsection{Corrigé}

 



\end{document}
