\documentclass[11pt,twoside,a4paper]{report}
%=========================== En-Tete =================================
%--- Insertion de paquetages (optionnel) ---
\usepackage[french]{babel}   % pour dire que le texte est en franÃ§ais
\usepackage{a4}              % pour la taille   
\usepackage[latin1]{inputenc}     % pour les font postscript
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{amsmath} 
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{xcolor}


\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{myred}{rgb}{0.6,0,0}
\definecolor{myblue}{rgb}{0,0,0.6}

\begin{document}
\lstset{%
backgroundcolor=\color{white}, 
basicstyle=\footnotesize,
commentstyle=\color{mygreen},
stringstyle=\color{myred},
breaklines=true,
frame=shadowbox,
keepspaces=true,
columns    = flexible,%
keywordstyle=\color{myblue},
language=Python,
numbers=left,
numbersep=5pt,
numberstyle=\tiny\color{black},
stringstyle=\color{mymauve}, 
xleftmargin=\parindent,
tabsize=3,
}%

\title{''Cours BigData MapReduce, une étude de cas''}
\author{''Benoit Petitpas''} 

\maketitle


\chapter{TP n°1}


\section{Problématique}


Lorsque les problématique BigData sont abordés, dans la littérature et sur le net, il parait assez clair que les problématiques BigData se scindent en deux grandes catégories:
\begin{enumerate}
\item \underline{Technologique}, car pour faire du BigData, il faut un ensemble d'infrastructures matériels (Cluster, Amazon EC2, EMR, ...) et logiciels (Hadoop, NoSQL databases, ...). Qui ont pour points commun d'être en perpetuelle évolution et d'avoir une littérature plutôt bien fournie sur le net.
\item \underline{Algorithmique}, car pour pouvoir faire du BigData, il faut pouvoir intéragir avec les données dans l'ensemble des infrastructures. Pour cela, la création d'algorithmes MapReduce est nécessaire. Des alternatives existent (Spark, Mahout, Hive ...) pour pouvoir manipuler les données sans passer par l'écriture d'algorithmes MapReduce, mais ne permettent pas des traitements avancés et personalisés.
\end{enumerate}

Ces technologies étant en perpetuelle évolution, leur simple suivi tecnique reste un challenge. Ainsi seuls des processus simples sont utilisés. Les limitations des processus batchs MapReduce sur Hadoop tendent encore à faire penser qu'Hadoop ne sert surtout qu'à faire de l'ETL, pour un moindre coût.\\
Un processus ETL (Extract, Transform, Load) consiste à extraire les données de fichiers de données non-structurés hétérogènes et arrivant en flux continu, puis à transformer ses données dans une certaine structuration, puis à les charger dans un DataWareHouse sur lequel les processus métiers seront executer.\\

Il semble clair, qu'une étude approfondie des possibilités algorithmiques MapReduce reste encore à faire. Le net dans ce cas précis manque singulièrement de littératures sauf si vous compter compter des mots encore et encore.\\

Le cout de compréhension du paradigme MapReduce est encore trop grand pour que les chercheurs et ingénieurs aient approfondis le sujet.\\

Voilà pourquoi le cours sur MapReduce prend la place d'un simple cours sur les technologies BigData.\\

\section{SQL sur des données non-structurés en MapReduce}

L'une des particularités des technologies BigData et notamment Hadoop est qu'elles reposent sur des information non-structurés stockées en HDFS (Hadoop File System) pour Hadoop. Or, il est courant de vouloir analyser les données avant d'y appliquer le moindre traitement.\\
Dans une base de données relationnelles (MySQL, PostGreSQL, ...), cette analyse se fait au moyen de requêtes SQL. \\

Il serait ainsi souhaitable de pouvoir faire les mêmes requêtes qu'en SQL sur des données non structurées. Car des données non-structurées ne peuvent être ni parcourues, ni analysées. Il sera donc nécessaire de structurer les données avant de les analyser.

\section{SQL pour les nuls}

\subsection{Introduction}

Il ne s'agit pas ici de faire un cours sur le SLQ mais plutôt de revoir les bases des commandes de manipulation de données. Les instructions de description de données seront passées sous silence.\\

\subsection{Instructions en SQL}

\begin{lstlisting}
SELECT * FROM table
WHERE nom_champs operateur valeur
GROUP BY nom_champs
ORDER BY nom_champs;
\end{lstlisting}

Une partie des instructions les plus usuelles pour faire de l'analyse de données sont :\\

\begin{enumerate}
\item $SELECT$ : Permet de selectionner des colonnes des tables, appelées champs (* selctionne tous les champs) ou de renvoyer des fonctions d'agrégats via un calcul:
\begin{itemize}
\item $AVG(nom champs)$ : permet de calculer la valeur moyenne des valeurs contenues dans le champ.
\item $COUNT(nom champs)$ : permet de calculer le nombre d'enregistrements (nombre de lignes)
\item $MIN/MAX(nom champs)$ : permet de calculer la valeur minimale/maximale contenu dans le champs.
\item $SUM(nom champs)$ : permet de calculer la somme des valeurs du champs.
\end{itemize}
\item WHERE : Permet de filtrer les enregistrements, c'est-à-dire que les valeurs contenus dans un champs seront toutes comparer à une valeur de filtrage. Par exemple : "nombre < 20". Les opérateurs de comparaison sont : "<, >, ==, IN, LIKE, ...". Chaque comparaison doit pouvoir renvoyer "VRAI" ou "FAUX". \\
On peut aussi chainer les comparaisons qui devra ne renvoyer qu'une seule valeur Vrai ou Faux. Par exemple, "nombre < 20 AND nombre > 10" comparera toutes les valeurs contenues dans le champs nombre et ne renverra que celles strictement supérieures 10 et inférieurs à 20. Pour chainer, il est courant d'utiliser les mots-clefs "AND" et "OR".
\item GROUP BY : permet de grouper plusieurs résultats et d'utiliser une fonction d'agrégat sur un groupe de résultats. Dans une table qui contiendrait toutes les ventes d'un magasin, il serait possible de regrouper les ventes par clients et d'obtenir le coût total des ses achats. 
\item ORDER BY : permet de trier les résultats.
\end{enumerate} 

\subsection{Exemple d'utilisation du SQL}

Soit la table "client" suivante :\\

\begin{tabular}{|c|c|c|c|}
\hline
nom & produit & prix & année\\
\hline
toto & a & 10 & 2014\\
toto & b & 20 & 2014\\
titi & a & 10 & 2013\\
titi & c & 50 & 2012\\
toto & a & 11 & 2013\\
titi & a & 11 & 2013\\
toto & d & 30 & 2014\\
\hline
\end{tabular}

\begin{enumerate}
\item Afficher les produits vendus : \\
 
\begin{lstlisting}
SELECT produit FROM client;
\end{lstlisting}

\item Afficher les ventes de plus de 15 euros:\\
 
\begin{lstlisting}
SELECT prix FROM client
WHERE prix > 15;
\end{lstlisting}

\item Afficher les ventes entre avant 2011 et après 2013:\\
 
\begin{lstlisting}
SELECT * FROM client
WHERE année < 2011 OR année > 2013;
\end{lstlisting}
(Si l'on met AND alors il faut qu'une année soit en même temps inférieure à 2011 et supérieure à 2013, ce qui est un ensemble nul).

\item Afficher le plus gros achat de chaque client :\\
 
\begin{lstlisting}
SELECT MAX(prix) FROM client
GROUP BY nom;
\end{lstlisting}

\item Afficher le nombre de ventes de chaque produit:\\
 
\begin{lstlisting}
SELECT COUNT(*) FROM client
GROUP BY produit;
\end{lstlisting}

\item Afficher la somme des ventes suppérieures à 15 euros par année:\\
 
\begin{lstlisting}
SELECT SUM(prix) FROM client
WHERE prix > 15
GROUP BY année;
\end{lstlisting}
\end{enumerate} 

\section{Map-Reduce SQL}

Même s'il existe des moyens de faire des requêtes en SQL sur des fichiers stockés en HDFS (comme Hive, Pig), les requêtes sur les données se feront par l'intermédiaire d'algorithmes Map-Reduce.\\
Pour cela nous allons utiliser un fichier CSV de données de ventes assez simple ayant pour structure : nom,produit,prix\_vente,magasin,pays,mois,année\\
Nous allons donc tenter de faire du requetage sur ces données.

\subsection{MapReduce pour les nuls}

MapReduce est un paradigme algorithmique. C'est-à-dire un ensemble de contraintes informatiques impliquant un effort algorithmique pour pouvoir se conformer à l'ensemble de contraintes.\\
Ces contraintes, sont simples : 
\begin{itemize}
\item Un algorithme MapReduce contient une phase Map, prenant en entrée une paire clef-valeur et en sorti une paire clef-valeur.
\item Un algorithme MapReduce contient une phase de triage, mélangeant les résultats des différents mapper et trie les résultats par clef. En entrée, le trieur prend une liste de paire clef-valeur et renvoie en sorti une liste de pair clef liste de valeur.
\item Un algorithme MapReduce contient une phase Reduce, prenant en entrée une clef et une liste de valeur et renvoie une clef et une valeur.
\end{itemize}

\begin{figure}[!h]
\includegraphics[width=10cm]{tp1_mapreduce.png} 
\caption{Exemple de processus MapReduce dans son application classique} 
\label{tp1_exo1_val} 
\end{figure} 

\subsection{Contexte technique}

Hadoop accepte des processus MapReduce écrit en Java. Hadoop grace à l'encapsulation de streamer permet de lancer des processus écris en python ou d'autres langages. Malgré tout, l'utilisation, configuration d'hadoop reste quelque chose d'assez sensible et l'écrire d'un processus MapReduce demande une certaine maitrise du langage. Dans un but pédagogique et pour que l'essentiel du temps ne soit pas passé à résoudre des problèmes de programmations, cette solution ne sera pas abordée. Sachez cependant qu'une maitrise d'hadoop peut-être interessante et que chaque exercice pourra être refait sur hadoop.\\

D'autres possibilités s'offraient à nous comme Sparks mais nécessitait une configuration et mise en place couteuse, même si son utilisation est plus simple et possède d'autres fonctionnalités. Ainsi une librairie Python a été choisi : MRJob, car son installation est simple, aucune configuration n'est demander pour tourner sur une machine locale et qu'elle est très bien documenté. \\

De plus, sa fonctionnalité de chainage de processus MapReduce est séduisante dans le cadre d'un cours.\\
MRJob est extremement pratique pour écrire simplement des processus MapReduce mais est particulièrement lent (10 fois plus lent qu'Hadoop). Ainsi MRJob permet de se concentrer sur le prototypage de processus MapReduce, permet d'être tester sur des cluster (un fichier de configuration permet de lancer les processus MRJob sur un cluster hadoop ou directement sur Amazon EMR).\\

Ainsi Pour écire un processus MapReduce il faudra une base : 

\begin{lstlisting}
# first thing to do is to load the lib
from mrjob.job import MRJob
# MRJob is a python class which will be overloaded

class SQLSelect(MRJob):
	def mapper(self, _, lines):
       #define your mapper
       yield(key,value)

	def reducer(self, key, values):
       #define your reducer
       yield(key,value) # or return, or print

if __name__ == '__main__':
	#just run mapreduce 
	SQLSelect.run()
\end{lstlisting}

\subsection{Exercice 1}

Afficher toutes les informations contenues dans le fichier. 

\begin{lstlisting}
SELECT * FROM client;
\end{lstlisting}

Indication : Pensez à ce que doivent être les clefs-valeurs en sortie du mapper.\\

Le but est de prendre tous les éléments du fichier et de les afficher (en gardant les information de lignes...).\\

\subsubsection{Corrigé}

Pour cela deux possibilités : \\

\underline{1 solution : passer les informations comme clef}

Map : Cette étape doit correspondre à la séparation du fichier en lignes envoyées à chaque mapper. Pour cela, le fichier est séparé en lignes. Chaque ligne devient une clef, la valeur est nulle car on ne compte rien en faire.\\

Reduce : Récupère un ensemble de clef-(liste de valeurs) trié par clef et affiche la clef, car la valeur est nulle.\\

 
\begin{lstlisting}
# first thing to do is to load the lib
from mrjob.job import MRJob
# MRJob is a python class which will be overloaded

class SQLSelect(MRJob):
	SORT_VALUES = True
	# send the line as a key and no value
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			yield(l,None)

	# and print it ... That's all folks 
	def reducer(self, k, v):
		print(k)

if __name__ == '__main__':
	#just run mapreduce 
	SQLSelect.run()
\end{lstlisting}

\underline{1 solution : passer les informations comme valeur}

Map : Cette étape doit correspondre à la séparation du fichier en lignes envoyées à chaque mapper. Pour cela, le fichier est séparé en lignes. Chaque ligne devient une valeur, la clef est nulle car on ne compte rien en faire.\\

Reduce : Récupère une clef nulle et la liste des valeurs contenant toutes les lignes.\\

 
\begin{lstlisting}
# first thing to do is to load the lib
from mrjob.job import MRJob
# MRJob is a python class which will be overloaded

class SQLSelect(MRJob):
	SORT_VALUES = True
	# send the line as a value associed with no key
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			yield(None,l)

	# and print it ... That's all folks 
	def reducer(self, k, values):
        for v in values:
		    print(v)

if __name__ == '__main__':
	#just run mapreduce 
	SQLSelect.run()
\end{lstlisting}

La ligne 6 sert à trier non pas seulement sur les clefs mais aussi sur les valeurs. Optionnel ici, cette option reste cependant extremement util en certaine occasion.
La ligne 9 se sert de la librairie standard String de Python pour séparer le fichier en ligne car on se sert du caratère spéciale '$\backslash n$' comme séparateur. On itère pour traiter chaque ligne.\\
La ligne 10 est la plus compliqué si les générateurs ne sont pas connus. Un générateur en Python permet de ne pas calculer le résultat d'une fonction avant de le retourner mais à la place de retourner un générateur des résulats. Cela permet, moins d'accès I/O entre les fonction et surtout de ne pas surcharger la mémoire, car le générateur ne marche qu'une fois:

\begin{lstlisting}
def print_ml(list):
   for item in list:
      print item
      # toute la liste est charger en mémoire pour affichage

def generate_list(list):
   for item in list:
      yield item
      # rien n'est charger, on créer un générator qui aura pout tache de parcourir toute la liste en oubliant au fuir et à mesure pour ne pas surcharger la mémoire

l = [1,2,3]

print_ml(l)
# affichera 1 2 3
print_ml(l)
# re affichera 1 2 3 car la liste est chargé en mémoire

list = generate_list(l)
for i in list:
   print i
   # affichera 1 2 3

for i in list:
   print i
   # n'affichera rien car une fois générer les valeurs sont oubliées, il faudra rappeller la fonction pour réafficher 1 2 3
\end{lstlisting}



\subsubsection{Différences}

En réalité, seule la deuxième solution correspond parfaitement à la commande SQL demandée, car elle renverra TOUS les enregistrements, même les doublons. Même si cette solution n'est pas du tout orienté BigData (car une seule machine parcourera tout le fichier).\\
Dans la deuxième solution, les mapper renvoient tous la même clef, ce qui implique qu'en sorti du trieur, il n'y ai qu'un seul reducer qui a pour entrée la clef nulle et une liste de toutes les valeurs. Le reducer boucle sur la liste pour afficher toutes les valeurs.\\

La première solution quand à elle ne répond pas vraiment à la demande car s'il existe une redondance des informations, ici une même vente effectuée, alors l'étape de sorting va regrouper les valeurs nulles sous une seule et même clef correspondant à l'information qui sera affichée.\\
Dans le cas d'un doublons, un reducer pourrait récupérer la paire clef-valeur : <$v_k$, ($\oslash$,$\oslash$)>, ainsi des informations seront perdus.\\

Remarque : La première solution répond plutôt à la requête :\\

\begin{lstlisting}
SELECT DISTINCT * FROM client;
\end{lstlisting}

Ici le mot clef DISTINCT permet d'éviter d'avoir des redondances d'information.\\

\begin{figure}[!h]
\includegraphics[width=10cm]{tp1_exo1_val.png} 
%\caption{Map, sort step with output of sorting step for the value oriented solution} 
\caption{Etapes map et sort pour le cas où les informations sont envoyées via la valeur} 
\label{tp1_exo1_val} 
\end{figure} 

\begin{figure}[!h]
\includegraphics[width=10cm]{tp1_exo1_key.png} 
%\caption{Map, sort step with output of sorting step for the value oriented solution} 
\caption{Etapes map et sort pour le cas où les informations sont envoyées via la clef} 
\label{tp1_exo1_key} 
\end{figure} 

On remarque qu'à la sortie de l'étape de triage, soit l'on obtient la liste des informations qui seront traitées dans un seul et unique reduceur, soit deux liste de valeurs nulle qui seront traités dans deux reduceurs différents.


\subsection{Exercice 2}

Afficher les ventes :\\

En SQL :
 
\begin{lstlisting}
SELECT prix FROM client;
\end{lstlisting}

Indication : Même exercice que précedemment, sauf qu'un filtrage des champs doit être effectué.\\
Pour cela penser qu'une ligne peut être de nouveau séparée, car un ligne est un objet String en Python.

\subsubsection{Corrigé}

Map : Cette étape doit correspondre à la séparation du fichier en lignes envoyées à chaque mapper. Pour cela, le fichier est séparé en lignes. Chaque ligne est de nouveau séparée pour récupérer le prix qui devient la clef.\\

Reduce : Récupère chaque ensemble de clef-valeurs et affiche les valeurs.\\

 
\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
            # knowing that price is the third column, just take this value
			yield(None,l.split(',')[2]) # the first element is 0

	def reducer(self, key, values):
        for v in values: 
            # values contains all the values, because they were all sorted to the None key
		    print(v)
\end{lstlisting}

\subsection{Exercie 3}

Afficher les prix de ventes :\\

En SQL :
 
\begin{lstlisting}
SELECT DISTINCT prix FROM client;
\end{lstlisting}

Indication : pour réussir très facilement cet exercice, une reflexion sur les deux exercices précédents est suffisante.\\

\subsubsection{Corrigé}

Pour réussir, il suffit de se servir de l'étape de sorting entre map et reduce et du fait que les valeurs soient triées par clef, ainsi si les prix de ventes sont des clefs ... 
 
\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
            # knowing that price is the third column, just take this value
			yield(l.split(',')[2],None) # the first element is 0

	def reducer(self, key, values):
        print(k) # values contains a list of None values !
\end{lstlisting}


\subsection{Exercice 4}

Afficher les ventes avant 2011:

 
\begin{lstlisting}
SELECT * FROM client
WHERE année < 2011;
\end{lstlisting}

Indication : ici, on reprend l'exercice 1 et on ajoute un filtrage sur les lignes et non plus les colonnes. Pour cela deux choix s'offrent à vous ... Soit vous souhaiter filtrer au niveau du mapper, soit du reducer. \\

Attention : la logique MapReduce indique que plus de ressources sont allouées pour les mapper (souvent plus nombreux). Ainsi seule cette solution sera présentée.\\

\subsubsection{Corrigé}

Ici l'idée est de ne mapper que les informations correspondantes au filtre en question, ici de vérifeir que l'année est inférieur à 2011.\\

 
\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			if int(l.split(',')[6]) < 2011:
				yield(None,l)

	def reducer(self, k, v):
		for i in v:
			print i
\end{lstlisting}


\subsection{Exercice 5}

Afficher l'argent amassé au cours des années passées. Puis l'argent amassé depuis 2012 inclu.

 
\begin{lstlisting}
SELECT SUM(prix) FROM client;
\end{lstlisting}

 
\begin{lstlisting}
SELECT SUM(prix) FROM client
WHERE annee >=2012;
\end{lstlisting}

\subsubsection{Corrigé}

Pour sommer les éléments il suffit de les additionner dans le reduceur.\\

 
\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			if int(l.split(',')[6]) > 2011:
				yield(None,float(l.split(',')[2]))

	def reducer(self, k, v):
		somme = 0
		for i in v:
			somme += i
		print somme
\end{lstlisting}

ou dans le cas où l'on connait un minimum python:\\

 
\begin{lstlisting}
  def mapper(self, _, lines):
     for l in lines.split('\n'):
        if int(l.split(',')[6]) > 2011:
           yield(None,float(l.split(',')[2]))

  def reducer(self, k, v):
     print sum(v)
\end{lstlisting}

On remarque l'utilisation float(), car les valeurs lues sont toujours considérées comme des chaines de caractères. On aurait aussi pu choisir de caster (terme informatique pour parler du passage forcé d'un type de donéne vers un autre).

Remarque sur le fichier de test le résultat est : 192941.5

Remarque bis: Souvenons nous que le mapper renvoie un générateur python qui n'est utilisable qu'une seule fois. Ce qui est suffisant ici mais si l'on avait voulu chainer deux calculs sur le reducer :\\

 
\begin{lstlisting}
	def reducer(self, k, v):
		print sum(v)
		print sum(v) # it's not very smart
\end{lstlisting}

Vous auriez obtenu : 192941.5 et 0 

\subsection{Exercice 6}

Afficher le nombre de vente.

 
\begin{lstlisting}
SELECT COUNT(*) FROM client
\end{lstlisting}

\subsubsection{Corrigé}

Pour compter les ventes, il suffit de connaitre le nombre de ligne du fichier. Pour cela on peut se servir des résultats de l'exercice 1.

 
\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
           yield(None,l)

	def reducer(self, k, v):
		cpt = 0
		for i in v:
			cpt += 1
		print cpt
\end{lstlisting}

La même chose avec des connaissances en python :\\

 
\begin{lstlisting}
   def mapper(self, _, lines):
      for l in lines.split('\n'):
         yield(None,l)

   def reducer(self, k, v):
      print len(list(v))
\end{lstlisting}

Le reduceur prend en entrée, une clef ainsi qu'un générateur de données python. La fonction list() permet de transformer un générateur en liste. Cette liste peut alors être compter avec la fonction len() qui renvoie la longeur d'une liste.\\

L'autre méthode est d'utiliser d'autres propriétés du paradigme MapReduce : \\

 
\begin{lstlisting}
	def mapper(self, _, lines):
		for l in lines.split('\n'):
			yield(None,1)

	def reducer(self, k, v):
		print sum(v)
\end{lstlisting}

Puisque l'étape de sort trie par clef, il suffit de donner la même valeur de clefs à chaque mapper ET de renvoyer 1 pour dire que la ligne à été traité. Ainsi le reduceur reçoit une clef vide avec une liste de valeur 1.


\subsection{Exercice 7}

Maintenant on voudrait connaitre le nombre de vente par année :\\

 
\begin{lstlisting}
SELECT COUNT(*) FROM client
GROUP BY année;
\end{lstlisting}

Indication : Ici, on entre dans les véritables utilisations de MapReduce, c'est-à-dire de la véritable analyse de données. Ainsi, pour résoudre ce problème il suffit de penser à quelle clef vous voullez en sortie de l'étape de triage.\\

\subsubsection{Corrigé}

Map : Pour chaque vente, l'année de la vente sera affectée comme clef au reduceur, et puisque l'on veu le nombre de vente il suffit de mettre la valeur à 1.

Reduce : Le reduceur n'a qu'à sommer les valeurs associées à chaque clef.

 
\begin{lstlisting}
def mapper(self, _, lines):
   for l in lines.split('\n'):
      yield(l.split(',')[6],1)

def reducer(self, k, v):
   print "l'année %s, il y a eu %s vente"%(k,sum(v))
\end{lstlisting}

Remarque : Avec le jeu de test vous devriez obtenir :\\

\noindent
\textsf{
l'année 1999, il y a eu 6299 vente\\
l'année 2000, il y a eu 6209 vente\\
l'année 2001, il y a eu 6174 vente\\
l'année 2002, il y a eu 6300 vente\\
l'année 2003, il y a eu 6181 vente\\
l'année 2004, il y a eu 6378 vente\\
l'année 2005, il y a eu 6305 vente\\
l'année 2006, il y a eu 6352 vente\\
l'année 2007, il y a eu 6208 vente\\
l'année 2008, il y a eu 6214 vente\\
l'année 2009, il y a eu 6182 vente\\
l'année 2010, il y a eu 6204 vente\\
l'année 2011, il y a eu 6318 vente\\
l'année 2012, il y a eu 6307 vente\\
l'année 2013, il y a eu 6201 vente\\
l'année 2014, il y a eu 6168 vente\\
}

\subsection{Exercice 7}

Afficher la somme des ventes par mois qui ne finissent pas par "bre":\\

\begin{lstlisting}
SELECT SUM(prix) FROM client
GROUP BY mois
HAVING mois NOT LIKE "%bre";
\end{lstlisting}

Ici on introduit un nouveau mot clef SQL "HAVING" qui est pour faire simple, une condition WHERE sur le champs du GROUP BY.\\

Indication : ici le problème est de filtrer les clefs pour ne pas les envoyer ou alors de les treier dans le reducer.\\

\subsubsection{Corrigé}

Map : envoie le mois comme clef si n'appartient pas à la liste [septembre, octobre, novembre, decembre].\\

Reduce : Somme les valeurs.\\

\begin{lstlisting}
def mapper(self, _, lines):
   for l in lines.split('\n'):
      mois = l.split(',')[5]
      prix = l.split(',')[2]
      if mois not in ['Septembre', 'Octobre', 'Novembre', 'Decembre']:
          yield(mois,float(prix))

	def reducer(self, k, v):
		print "%s : %s euros"%(k,sum(v))
\end{lstlisting}

Les résultats alors ovbtenus devraient être : \\

\noindent
\textsf{
Avril : 92179.5 euros\\
Fevrier : 92953.0 euros\\
Janvier : 94369.5 euros\\
Juillet : 92419.0 euros\\
Juin : 93857.5 euros\\
Mai : 93006.5 euros\\
Mars : 91721.5 euros\\
}

\subsection{Exercice 8}

Afficher le meilleur client de 2013.\\

\subsubsection{Corrigé}

Il n'est pas possible de résoudre ce problème avec une seule étape de MapReduce. Deux étape de map reduce sont alors nécessaire, l'une pour compter la somme des achat par client puis de parcourir cette liste et de choisir le max. \\

MRJob permet très simplement d'enchainer plusieurs job MapReduce dans un seul programme. Pour cela, il suffit de définir des steps, en surchargeant la méthod step de la classe MRJob.

\begin{lstlisting}
class SQLSelect(MRJob):
   SORT_VALUES = True
   def mapper(self, _, lines):
      for l in lines.split('\n'):
         client = l.split(',')[0]
         prix = l.split(',')[2]
         yield(client,float(prix))

   def reducer_par_client(self, k, v):
      yield(None, (sum(v),k))

   def reducer_meilleur(self, _, v):
      best_kv = max(v)
      print "%s a dépensé %s euros !"%(best_kv[1],best_kv[0])

   def steps(self):
      return [
         self.mr(mapper=self.mapper,
                 reducer=self.reducer_par_client),
         self.mr(reducer=self.reducer_meilleur)
		]

if __name__ == '__main__':
	SQLSelect.run()
\end{lstlisting}

Vous devriez trouvez "Martinez a dépensé 12619.5 euros !"\\

\subsubsection{Explication}

Pour l'étape de Map, il n'y pas de difficultés, les clefs étant triées dans l'étape de triage, il parait évident qu'il faut utilisé les noms comme clef. Car dans ce cas, l'étape Reduce récupérera chaque nom avec l'ensemble des valeurs associées, ici les prix des commandes.\\
Le fait de forcer les prix en float (réel) permet d'utiliser la puissance de calcul des mappers.\\

Pour le premier reduceur, on applique les résultats de l'exercice précédent pour pouvoir appliquer le GROUP BY. Ainsi en sorti des premiers reducers, on trouve la somme des achats par client. Maintenant il faut pouvoir en extraire le plus grand. Un reducer par client sera nécessaire pour calcuelr les sommes des achats par client.\\

Le but étant de récupérer dans un deuxième reducer l'ensemble des couples clefs-valeurs (ici client-somme\_des\_achats) pour pouvoir itérer sur la liste des couples, il est donc primordiale que le premier reducer renvoie un couple clef valeur avec la même clef.\\
Pour résumer, le premier reducer ne sert qu'à préparer la concaténation des résultats qui aura lieux dans la deuxième étape de triage.\\

Ainsi à la ligne 10, on renvoie un générateur produisant des ensembles clef-valeur avec pour clef la valeur vide et pour valeur un tuple contenant les clients et la somme de leurs achats. Ici on inverse pour voir directement appliquer dans le deuxième reducer la fonction python Max(). Mais on aurait pu garder l'ordre initial :\\

\begin{lstlisting}
def reducer_meilleur(self, _, v):
   max = 0
   best_c = None
   for i in v:
      s_prix = i[1]
      if s_prix > max:
         max = s_prix
         best_c = i[0]

   print best_c, max
\end{lstlisting}

Car les valeur en entrée du deuxième reducer sont des tuples accessible via les crochets.\\
   
Ensuite comme expliquer plus tôt, il est impératif de surcharger la méthode de classe "steps()", pour cela, il suffit de définir les différent job MapReduce que l'on veut enchainer et de définir pour chaque job les méthode de map et de reduce.\\

Ici, un premier job est défini avec pour mapper et reducer nos fonctions permettant de calculer les achats par client.\\
Un deuxième job est alors créer avec uniquement un reduceur (dans ce cas il n'y aura pas de map, on récupère juste le résultat précédent) permettant de mesurer le max.\\


\subsection{Exercice 9}

Afficher les 10 mots les plus cités. 

\subsubsection{Corrigé}   
	
\begin{lstlisting}
	def mapper(self, _, line):
		words = line.split(',')
		for w in words:
			yield(w,1)

	def reducer_1(self, k, v):
		yield(None,(sum(v),k))
	
	def reducer_2(self,_,v):
		l_v= list(v)
		sorted(l_v,reverse=True)
		best = l_v[:10]
		for b in best:
			print "%s : %s"%(b[0],b[1])
\end{lstlisting}

Les difficultés commencent là, car comment récupérer les 10 meilleurs une fois les mots comptés. Pour cela on utilise un nouveau reducer qui va trier les résultats (car il est plus compliquer ici de trier les résultats par le sort de Map-Reduce). ici les difficultés ne sont pas algorithmique mais de programation Python. \\

La ligne 10 permet de récupérer les résultats dans une liste de tuples.\\
La ligne 11 permet de trier cette liste en utilisant la fonction "sorted" de Python. Reverse permet d'obtenir un ordre decroissant.\\
On peut remarque que si la sortie du reducer 1 avait été inversée, il aurait été nécessaire de spécifier sur quelle "clef" (dans un sens différent de la clef de la pair clef-valeur) trier. Pour cela, une fonction anonyme aurait dut être écrite (lambda) qui à un tuple renvoie son deuxième élément. Ansi la clef de triage aurait été le deuxième élément de chaque tuple.\\

Enfin on ne récupère que les 10 premières valeurs de la liste qu'on affiche ensuite.\\
\end{document}  
